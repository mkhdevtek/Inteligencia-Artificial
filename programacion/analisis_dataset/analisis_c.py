import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from wordcloud import WordCloud

# Si usas nltk para an√°lisis m√°s avanzado
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    NLTK_AVAILABLE = True
except:
    NLTK_AVAILABLE = False

# 1. CARGA Y EXPLORACI√ìN INICIAL
def cargar_corpus(ruta_csv, columna_texto, encoding='utf-8'):
    """Carga el dataset CSV"""
    df = pd.read_csv(ruta_csv, encoding=encoding)
    print(f"üìä Dataset cargado: {len(df)} documentos")
    print(f"üìã Columnas disponibles: {list(df.columns)}")
    return df

def exploracion_basica(df, columna_texto):
    """An√°lisis exploratorio b√°sico del corpus"""
    print("\n" + "="*60)
    print("EXPLORACI√ìN B√ÅSICA DEL CORPUS")
    print("="*60)
    
    # Estad√≠sticas generales
    df['longitud'] = df[columna_texto].astype(str).str.len()
    df['num_palabras'] = df[columna_texto].astype(str).str.split().str.len()
    
    print(f"\nüìù Total de documentos: {len(df)}")
    print(f"üìä Caracteres por documento:")
    print(f"   - Promedio: {df['longitud'].mean():.0f}")
    print(f"   - M√≠nimo: {df['longitud'].min()}")
    print(f"   - M√°ximo: {df['longitud'].max()}")
    print(f"\nüí¨ Palabras por documento:")
    print(f"   - Promedio: {df['num_palabras'].mean():.1f}")
    print(f"   - M√≠nimo: {df['num_palabras'].min()}")
    print(f"   - M√°ximo: {df['num_palabras'].max()}")
    
    return df

# 2. LIMPIEZA Y PREPROCESAMIENTO
def limpiar_texto(texto):
    """Limpia y normaliza el texto"""
    texto = str(texto).lower()
    # Eliminar URLs
    texto = re.sub(r'http\S+|www\S+|https\S+', '', texto)
    # Eliminar menciones y hashtags (opcional)
    texto = re.sub(r'@\w+|#\w+', '', texto)
    # Eliminar caracteres especiales y n√∫meros
    texto = re.sub(r'[^a-z√°√©√≠√≥√∫√±√º\s]', '', texto)
    # Eliminar espacios m√∫ltiples
    texto = re.sub(r'\s+', ' ', texto).strip()
    return texto

def preprocesar_corpus(df, columna_texto):
    """Preprocesa todo el corpus"""
    print("\nüîß Preprocesando textos...")
    df['texto_limpio'] = df[columna_texto].apply(limpiar_texto)
    return df

# 3. AN√ÅLISIS DE FRECUENCIA DE PALABRAS
def analizar_frecuencias(df, columna_texto, top_n=20, stopwords_es=None):
    """Analiza frecuencia de palabras"""
    
    # Stopwords en espa√±ol
    if stopwords_es is None:
        stopwords_es = set([
            'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'ser', 'se', 'no',
            'haber', 'por', 'con', 'su', 'para', 'como', 'estar', 'tener',
            'le', 'lo', 'todo', 'pero', 'm√°s', 'hacer', 'o', 'poder', 'decir',
            'este', 'ir', 'otro', 'ese', 'la', 'si', 'me', 'ya', 'ver', 'porque',
            'dar', 'cuando', '√©l', 'muy', 'sin', 'vez', 'mucho', 'saber', 'qu√©',
            'sobre', 'mi', 'alguno', 'mismo', 'yo', 'tambi√©n', 'hasta', 'a√±o',
            'dos', 'querer', 'entre', 'as√≠', 'primero', 'desde', 'grande', 'eso',
            'ni', 'nos', 'llegar', 'pasar', 'tiempo', 'ella', 's√≠', 'd√≠a', 'uno',
            'bien', 'poco', 'deber', 'entonces', 'poner', 'cosa', 'tanto', 'hombre',
            'parecer', 'nuestro', 'tan', 'donde', 'ahora', 'parte', 'despu√©s', 'vida',
            'quedar', 'siempre', 'creer', 'hablar', 'llevar', 'dejar', 'nada', 'cada',
            'seguir', 'menos', 'nuevo', 'encontrar', 'algo', 'solo', 'decir', 'llamar'
        ])
    
    # Concatenar todos los textos
    texto_completo = ' '.join(df[columna_texto].astype(str))
    palabras = texto_completo.split()
    
    # Filtrar stopwords y palabras cortas
    palabras_filtradas = [p for p in palabras if p not in stopwords_es and len(p) > 2]
    
    # Contar frecuencias
    frecuencias = Counter(palabras_filtradas)
    palabras_comunes = frecuencias.most_common(top_n)
    
    print("\n" + "="*60)
    print(f"üî§ TOP {top_n} PALABRAS M√ÅS FRECUENTES")
    print("="*60)
    for palabra, freq in palabras_comunes:
        print(f"{palabra:20s} : {freq:5d} veces")
    
    return frecuencias, palabras_comunes

# 4. VISUALIZACIONES
def crear_wordcloud(frecuencias, titulo="Nube de Palabras"):
    """Crea una nube de palabras"""
    plt.figure(figsize=(12, 6))
    
    wordcloud = WordCloud(
        width=800, 
        height=400,
        background_color='white',
        colormap='viridis',
        max_words=100
    ).generate_from_frequencies(frecuencias)
    
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(titulo, fontsize=16, pad=20)
    plt.tight_layout()
    plt.show()

def grafico_barras_frecuencias(palabras_comunes, top_n=20):
    """Gr√°fico de barras de palabras m√°s comunes"""
    palabras, frecuencias = zip(*palabras_comunes[:top_n])
    
    plt.figure(figsize=(12, 8))
    plt.barh(range(len(palabras)), frecuencias, color='steelblue')
    plt.yticks(range(len(palabras)), palabras)
    plt.xlabel('Frecuencia', fontsize=12)
    plt.ylabel('Palabras', fontsize=12)
    plt.title(f'Top {top_n} Palabras M√°s Frecuentes', fontsize=14, pad=20)
    plt.gca().invert_yaxis()
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.show()

def analisis_longitud(df):
    """Visualiza distribuci√≥n de longitudes"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Histograma de longitud en caracteres
    axes[0].hist(df['longitud'], bins=30, color='skyblue', edgecolor='black')
    axes[0].set_xlabel('Caracteres')
    axes[0].set_ylabel('Frecuencia')
    axes[0].set_title('Distribuci√≥n de Longitud (caracteres)')
    axes[0].grid(alpha=0.3)
    
    # Histograma de n√∫mero de palabras
    axes[1].hist(df['num_palabras'], bins=30, color='lightcoral', edgecolor='black')
    axes[1].set_xlabel('Palabras')
    axes[1].set_ylabel('Frecuencia')
    axes[1].set_title('Distribuci√≥n de Longitud (palabras)')
    axes[1].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# 5. AN√ÅLISIS POR CATEGOR√çAS (si existen)
def analisis_por_categoria(df, columna_texto, columna_categoria):
    """Analiza el corpus por categor√≠as"""
    if columna_categoria not in df.columns:
        print(f"‚ö†Ô∏è  Columna '{columna_categoria}' no encontrada")
        return
    
    print("\n" + "="*60)
    print("üìë AN√ÅLISIS POR CATEGOR√çAS")
    print("="*60)
    
    for categoria in df[columna_categoria].unique():
        subset = df[df[columna_categoria] == categoria]
        print(f"\nüè∑Ô∏è  {categoria}")
        print(f"   Documentos: {len(subset)}")
        print(f"   Promedio palabras: {subset['num_palabras'].mean():.1f}")

# FUNCI√ìN PRINCIPAL
def analizar_corpus_completo(ruta_csv, columna_texto, columna_categoria=None):
    """Ejecuta an√°lisis completo del corpus"""
    
    # 1. Cargar datos
    df = cargar_corpus(ruta_csv, columna_texto)
    
    # 2. Exploraci√≥n b√°sica
    df = exploracion_basica(df, columna_texto)
    
    # 3. Limpiar textos
    df = preprocesar_corpus(df, columna_texto)
    
    # 4. An√°lisis de frecuencias
    frecuencias, palabras_comunes = analizar_frecuencias(df, 'texto_limpio', top_n=20)
    
    # 5. Visualizaciones
    print("\nüìä Generando visualizaciones...")
    grafico_barras_frecuencias(palabras_comunes, top_n=20)
    crear_wordcloud(frecuencias, "Nube de Palabras del Corpus")
    analisis_longitud(df)
    
    # 6. An√°lisis por categor√≠as (si existe)
    if columna_categoria:
        analisis_por_categoria(df, columna_texto, columna_categoria)
    
    return df, frecuencias

# EJEMPLO DE USO
if __name__ == "__main__":
    # Reemplaza con tu archivo CSV
    ruta_csv = './datasetTexto.csv'  # Tu archivo
    columnas_texto = [
        'Titulo',
        'Medio',
        'Resumen',
        'Comentario_Reaccion'
    ] # Nombre de la columna con el texto
    columna_categoria = 'Categoria'       # Opcional: columna de categor√≠as
    
    df_analizado = ""

    for columna_texto in columnas_texto:
        # Ejecutar an√°lisis
        df_analizado, frecuencias = analizar_corpus_completo(
            ruta_csv, 
            columna_texto,
            columna_categoria  # Puede ser None si no tienes categor√≠as
        )
    
    # Guardar resultados
    df_analizado.to_csv('corpus_analizado.csv', index=False)
    print("\n‚úÖ An√°lisis completado y guardado!")
